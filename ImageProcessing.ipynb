{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# データの拡張\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNetからのトレーニング済みモデルのダウンロード\n",
    "[Keras Documentation](https://keras.io/ja/applications/#vgg16)  \n",
    "[Keras：VGG16、VGG19とかってなんだっけ？？](https://qiita.com/MuAuan/items/86a56637a1ebf455e180)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 59MBのダウンロードが必要\n",
    "pre_model = keras.applications.VGG16(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベースモデルの凍結\n",
    "[学習済みCNNモデルの利用方法まとめ【ディープラーニング】](https://sinyblog.com/deaplearning/pre_train_model/)  \n",
    "[TensorFlow, Kerasでレイヤー、モデルのtrainable属性を設定（Freeze / Unfreeze）](https://note.nkmk.me/python-tensorflow-keras-trainable-freeze-unfreeze/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false で凍結\n",
    "pre_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新しいレイヤーの追加\n",
    "[Keras Documentation](https://keras.io/ja/layers/pooling/)  \n",
    "[全体平均プーリング（1/5）背景技術](https://benrishi-ai.com/gapooling01/)  \n",
    "[Global Average Pooling（GAP）を理解してみる](https://qiita.com/mine820/items/1e49bca6d215ce88594a)\n",
    "[Keras Documentation Dense](https://keras.io/ja/layers/core/)  \n",
    "[Keras Documentation 活性化関数の使い方](https://keras.io/ja/activations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### keras.Input()とは？\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = image_model(inputs, training=False)\n",
    "\n",
    "# 空間データのグローバルな平均プーリング演算\n",
    "# 空間データとは「モノの位置情報と意味情報（状態、形状、サイズなど）の双方を要素として持つデータ」であり、\n",
    "# 具体的には「車両が交差点を5km/hで走行している」というようなものを指します。\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Dense 密集\n",
    "# 通常の全結合ニューラルネットワークレイヤー\n",
    "\n",
    "# units：出力空間の次元数\n",
    "units = 6\n",
    "# activation は 活性関数\n",
    "outputs = keras.layers.Dense(units, activation = 'softmax')(x)\n",
    "\n",
    "# \n",
    "revised_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのコンパイル\n",
    "[loss 損失関数](https://keras.io/api/losses/) モデルがトレーニング中に最小化しようとする量を計算すること  \n",
    "損失関数は予測と実際の値のズレの大きさを表す関数でモデルの予測精度を評価します。損失関数の値が小さければより正確なモデル  \n",
    "\n",
    "交差エントロピー誤差\n",
    "この損失関数は分類問題に用います。以下の数式で表されるのが交差エントロピー誤差  \n",
    "\n",
    "$$\n",
    "    E=-\\logyk\n",
    "$$\n",
    "\n",
    "[機械学習における損失関数の役割や種類](https://ai-trend.jp/basic-study/neural-network/nn_loss_function/#:~:text=%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%92%E3%81%AF%E3%81%98%E3%82%81%E3%81%A8,%E3%81%AB%E8%80%83%E3%81%88%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%97%E3%82%87%E3%81%86%E3%80%82)\n",
    "\n",
    "merics 評価関数 訓練時とテスト時にモデルにより評価される評価関数のリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics = [keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=True,  # set each sample mean to 0\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True # we don't expect Bo to be upside-down so we will not flip vertically\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \n",
    "test_data_path = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and iterate training dataset\n",
    "train_it = datagen.flow_from_directory(train_data_path,\n",
    "                                       target_size=(244, 244), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode=\"categorical\")\n",
    "# load and iterate test dataset\n",
    "test_it = datagen.flow_from_directory(test_data_path,\n",
    "                                      target_size=(244, 244), \n",
    "                                      color_mode='rgb', \n",
    "                                      class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのトレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=test_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=test_it.samples/test_it.batch_size,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの凍結を解除してファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 凍結の解除\n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンパイル\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = .00001),\n",
    "              loss = keras.losses.BinaryCrossentropy(from_logits=True) , metrics =  [keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "model.fit(train_it,\n",
    "          validation_data=test_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=test_it.samples/test_it.batch_size,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_it, steps=test_it.samples/test_it.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image)\n",
    "\n",
    "def make_predictions(image_path):\n",
    "    show_image(image_path)\n",
    "    image = image_utils.load_img(image_path, target_size=(224, 224))\n",
    "    image = image_utils.img_to_array(image)\n",
    "    image = image.reshape(1,224,224,3)\n",
    "    image = preprocess_input(image)\n",
    "    preds = model.predict(image)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = make_predictions('fruits/test/freshapples/Screen Shot 2018-06-08 at 5.27.54 PM.png')\n",
    "np.argmax(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判定結果を表示する関数\n",
    "def tasting_fruits(image_path):\n",
    "    preds = make_predictions(image_path)\n",
    "    if np.argmax(preds)>=4:\n",
    "        print(\"It's ratten! Not yammy! Boo!\")\n",
    "    else:\n",
    "        print(\"It's fresh! Yammy! Yes!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
